{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6814563",
   "metadata": {},
   "source": [
    "Note : les lignes de commandes présentées ici sont valides pour des systèmes Linux avec un packet manager basé sur Debian (comme Ubuntu), ce qui corresponds aux distributions principales Linux. Si vous n'avez ce type de système et que vous ne souhaitez pas l'installer en double boot ou en machine virtuelle, vous devrez rechercher des équivalents pour votre système d'exploitation. \n",
    "\n",
    "# 2 - Manipuler les textes et les symboles\n",
    "\n",
    "## 2.1 - Segmentation des symboles (tokenization)\n",
    "\n",
    "Nous allons ici explorer les différentes manières de segmenter les textes avec les librairies les plus communes de NLP. Dans un premier temps il faut installer Python 3 et la librairie NLTK avec la commande suivante :\n",
    "\n",
    "sudo apt install python3 python3-dev python3-pip ipython3 python-is-python3 build-essential \n",
    "\n",
    "Le paquet python-is-python3 sert à créer des alias pour les commandes python3 et pip3 en enlevant le numéro de version pour plus de simplicité. Ensuite, installez les paquets suivants :\n",
    "\n",
    "pip install nltk spacy datasets charset-normalizer autocorrect cleantext emot wordfreq textblob torchdata torchtext scikit-plot lime stanza sentence-transformers transformers torch gensim evaluate\n",
    "\n",
    "Le paquet nltk a des données optionnelles téléchargables, comme des modèles pré-entrainés, des datasets ainsi que des dictionnaires de mots ou de symboles. Par simplicité et pour éviter d'être bloqué sur certaines parties, nous allons tout télécharger. Pour cela, lancez le terminal Python dans un terminal :\n",
    "\n",
    "ipython # ou ipython3 si ça ne fonctionne pas\n",
    "\n",
    "Cela va lancer le terminal d'interprétation de code Python dans lequel vous taperez :\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "Une interface graphique va se lancer dans laquelle il vous suffira de télécharger le méta-paquet \"all\". Si vous utilisez un Colab (ce que je vous encourage à ne pas faire et à utiliser votre propre système Linux), il vaut mieux télécharger un à un les paquets : \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "Concernant Spacy, nous aurons besoin de quatre modèles qui sont eux aussi téléchargeable séparément :\n",
    "\n",
    "python -m spacy download fr_core_news_sm\n",
    "\n",
    "python -m spacy download fr_core_news_lg\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "python -m spacy download en_core_web_lg\n",
    "\n",
    "Les versions sm sont des petits modèles rapides et les versions lg sont des plus gros modèles plus lents mais plus performants.\n",
    "\n",
    "Concernant stanza, nous aurons également besoin de télécharger des modèles additionnels (avec ipython) :\n",
    "\n",
    "import stanza; stanza.download()\n",
    "\n",
    "Note : toute librairie istallée pendant l'utilisation de ce notebook ne pourraient être disponible tout de suite. Pour que Jupyter les prenne en compte, il convient de relancer le notebook. \n",
    "\n",
    "Note 2 : si vous manquez de place sur votre disque principal avec tous les modèles, repérez les dossiers où sont stockés les modèles dans votre répertoire home (gensim-data, nltk_data, .config/hugginface, stanza_resources ...). Déplacez ces dossiers un autre disque interne ou externe et remplacez les dossiers par des liens symboliques (ln -s <chemin_cible> <chemin_lien>) aux anciens chemins vers les nouvelles destinations.\n",
    "\n",
    "### Test des différentes manières de segmenter un text\n",
    "\n",
    "Nous allons travailler sur le dataset de review d'Amazon. Comme il nous aurons besoins d'autres librairies pour travailler sur les datasets et les manipuler, vous pouvez les installer maintenant si ce n'est pas déjà fait :\n",
    "\n",
    "pip install pandas datasets\n",
    "\n",
    "\n",
    "#### Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset('amazon_reviews_multi', split = 'train').to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fc58a",
   "metadata": {},
   "source": [
    "#### Exploration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662194f",
   "metadata": {},
   "source": [
    "#### Sélectionner un commentaire français de plus de 1000 caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa61324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr  = df[df.language == 'fr']\n",
    "review = None\n",
    "for _, rev in df_fr['review_body'].items():\n",
    "    if len(rev) > 1000:\n",
    "        review = rev\n",
    "        break\n",
    "        \n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ab9c6",
   "metadata": {},
   "source": [
    "#### Décomposer le commentaire avec différents tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    TreebankWordTokenizer, \n",
    "    ToktokTokenizer, \n",
    "    TweetTokenizer,\n",
    "    WhitespaceTokenizer\n",
    ")\n",
    "from nltk.tokenize import (\n",
    "    word_tokenize, \n",
    "    wordpunct_tokenize, \n",
    "    sent_tokenize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ced94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(review)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = wordpunct_tokenize(review)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85777eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sent_tokenize(review)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens    = tokenizer.tokenize(review)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "tokens    = tokenizer.tokenize(review)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8081465",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "tokens    = tokenizer.tokenize(review)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ead94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens    = tokenizer.tokenize(review)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605272d",
   "metadata": {},
   "source": [
    "#### Construire un vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab     = set()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "for _, rev in df_fr['review_body'].items():\n",
    "    vocab.update(tokenizer.tokenize(rev))\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5821825",
   "metadata": {},
   "source": [
    "#### Enlever les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac76388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "vocab = vocab - set(stopwords.words('french'))\n",
    "vocab = vocab - set(stopwords.words('english'))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a965d",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer    = PorterStemmer()\n",
    "stem_vocab = [stemmer.stem(word) for word in vocab]\n",
    "print(stem_vocab[:50])\n",
    "\n",
    "stem_vocab = set(stem_vocab)\n",
    "print(len(stem_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "stem_vocab = [stemmer.stem(word) for word in vocab]\n",
    "print(stem_vocab[:50])\n",
    "\n",
    "stem_vocab = set(stem_vocab)\n",
    "print(len(stem_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer       = WordNetLemmatizer()\n",
    "lemmatizer_vocab = [lemmatizer.lemmatize(word) for word in vocab]\n",
    "print(lemmatizer_vocab[:50])\n",
    "\n",
    "lemmatizer_vocab = set(lemmatizer_vocab)\n",
    "print(len(lemmatizer_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09203cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "vocab = set()\n",
    "nlp   = spacy.load('fr_core_news_sm')\n",
    "\n",
    "for _, rev in df_fr['review_body'].items():\n",
    "    doc = nlp(rev)\n",
    "    for token in doc:\n",
    "        vocab.add(token.lemma_)\n",
    "        \n",
    "vocab = vocab - set(stopwords.words('french'))\n",
    "vocab = vocab - set(stopwords.words('english'))\n",
    "vocab = list(vocab)\n",
    "        \n",
    "print(vocab[:50])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12fe4e",
   "metadata": {},
   "source": [
    "## 2.2 - Détection de récurrences (patterns) et nettoyage de texte\n",
    "\n",
    "### Encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import charset_normalizer\n",
    "\n",
    "def decode_charset(raw_bytes, suggested_encoding = None):\n",
    "  if raw_bytes is None:\n",
    "    return None, None\n",
    "\n",
    "  decoded       = None\n",
    "  real_encoding = None\n",
    "  if suggested_encoding is not None:\n",
    "    try:\n",
    "      decoded       = raw_bytes.decode(suggested_encoding, 'strict')\n",
    "      real_encoding = suggested_encoding\n",
    "    except (BaseException, Exception, ArithmeticError, BufferError, LookupError):\n",
    "      matches = charset_normalizer.from_bytes(raw_bytes)\n",
    "      best    = matches.best()\n",
    "      if best is None:\n",
    "        return None, None\n",
    "\n",
    "      real_encoding = best.encoding\n",
    "      decoded       = raw_bytes.decode(real_encoding, 'ignore')\n",
    "  else:\n",
    "    matches = charset_normalizer.from_bytes(raw_bytes)\n",
    "    best    = matches.best()\n",
    "    if best is None:\n",
    "      return None, None\n",
    "\n",
    "    real_encoding = best.encoding\n",
    "    decoded       = raw_bytes.decode(real_encoding, 'ignore')\n",
    "\n",
    "  return decoded, real_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url  = 'https://martin.slouf.name/'\n",
    "page = urllib.request.urlopen(url)\n",
    "\n",
    "# the website responds with unidentified charset\n",
    "print('the page advertised encoding: ' + str(page.headers.get_content_charset()))\n",
    "\n",
    "# so we have bytes with unidentified encoding\n",
    "byte_content = page.read()\n",
    "\n",
    "# luckily we use decode_charset to decode the bytes as string\n",
    "html, encoding = decode_charset(byte_content)\n",
    "print('the page real encoding: ' + str(encoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8df771",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = b\"\"\"\n",
    "La longueur de l'hypoth\\xe9nuse au carr\\xe9 dans un triangle \n",
    "rectangle est \\xe9gale \\xe0 h\\xb2 = a\\xb2 + b\\xb2\n",
    "\"\"\"\n",
    "\n",
    "print(data.decode(\"utf-8\")) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac72904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real encoding\n",
    "data_str, encoding = decode_charset(data, suggested_encoding = 'latin-1') \n",
    "print(encoding)\n",
    "print(data_str)\n",
    "\n",
    "# encoding that works\n",
    "data_str, encoding = decode_charset(data) \n",
    "print(encoding)\n",
    "print(data_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35971c54",
   "metadata": {},
   "source": [
    "### Regex\n",
    "\n",
    "#### re.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49710242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = 'Mon adresse email est blabla@gmail.com'\n",
    "match = re.search(r'[a-z]+@[a-z]+\\.com', text)\n",
    "\n",
    "if match:\n",
    "  print('found', match.group())\n",
    "else:\n",
    "  print('did not find')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'Mon adresse email est blabla@gmail.com'\n",
    "text_2 = 'blabla@gmail.com est mon adresse mail'\n",
    "\n",
    "regex_begin = r'\\A[a-z]+@[a-z]+\\.com'\n",
    "regex_end   = r'[a-z]+@[a-z]+\\.com\\Z'\n",
    "\n",
    "if re.search(regex_begin, text_1):\n",
    "    print('email found at beginning in text_1')\n",
    "if re.search(regex_begin, text_2):\n",
    "    print('email found at beginning in text_2')\n",
    "if re.search(regex_end, text_1):\n",
    "    print('email found at the end in text_1')\n",
    "if re.search(regex_end, text_2):\n",
    "    print('email found at the end in text_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2089aa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's a match : romain@hotmail.com\n",
      "it's a match : jeanyves@yahoo.com\n",
      "it's a match : romain@hotmail.fr\n",
      "it's a match : jean.yves@yahoo.com\n",
      "STOOOOOP\n",
      "it's a match : jean-yves@yahoo.com\n",
      "no match : romain@hotmail.gmail.fr\n",
      "no match : roger@yahoo\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "do_match = [\n",
    "    'romain@hotmail.com',\n",
    "    'jeanyves@yahoo.com',\n",
    "    'romain@hotmail.fr',\n",
    "    'jean.yves@yahoo.com'\n",
    "]\n",
    "\n",
    "dont_match = [\n",
    "    'jean-yves@yahoo.com',\n",
    "    'romain@hotmail.gmail.fr',\n",
    "    'roger@yahoo'\n",
    "]\n",
    "\n",
    "regex = r'[a-z\\.]+@[a-z]+\\.+[a-z]{1,3}\\Z'\n",
    "\n",
    "for email in do_match:\n",
    "    if re.search(regex, email):\n",
    "        print(f'it\\'s a match : {email}')\n",
    "    else:\n",
    "        print(f'no match : {email}')\n",
    "\n",
    "\n",
    "print('STOOOOOP')\n",
    "for email in dont_match:\n",
    "    if re.search(regex, email):\n",
    "        print(f'it\\'s a match : {email}')\n",
    "    else:\n",
    "        print(f'no match : {email}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8211ff",
   "metadata": {},
   "source": [
    "#### Les quantifieurs de répétition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ee130",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_match = [\n",
    "    '1 file has been found',\n",
    "    '34 files has been found',\n",
    "    '2 directories has been found, 1 file has been found'\n",
    "]\n",
    "\n",
    "dont_match = [\n",
    "    '15 files has been found with similar search parameters, no results with current parameters',\n",
    "    'No files has been found',\n",
    "    'The files has not been found',\n",
    "    '5 files cound have been found, but the disk returned some I/O errors',\n",
    "    '1 directory has been found, no file has been found'\n",
    "]\n",
    "\n",
    "regex = r'<your_solution>'\n",
    "\n",
    "for res in do_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')\n",
    "        \n",
    "for res in dont_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ebdc3b",
   "metadata": {},
   "source": [
    "#### Classes et intervalles de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81767570",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "<dl>\n",
    "  <dt id=\"introduction_au_html\">Introduction au HTML</dt>\n",
    "  <dd>\n",
    "    <p>Vous faites vos premiers pas dans le développement web ? <a\n",
    "    href=\"/fr/docs/Learn/Getting_started_with_the_web/HTML_basics\">Nos articles\n",
    "    sur les bases de HTML</a> expliquent ce qu'est HTML et comment\n",
    "    l'utiliser.</p>\n",
    "  </dd>\n",
    "  <dt id=\"tutoriels_html\">Tutoriels HTML</dt>\n",
    "  <dd>\n",
    "    <p>Pour plus d'informations sur l'utilisation du HTML, des tutoriels et des\n",
    "    exemples complets, vous pouvez consulter <a href=\"/fr/docs/Learn/HTML\">notre\n",
    "    section Apprendre HTML</a>.</p>\n",
    "  </dd>\n",
    "  <dt id=\"référence_html\">Référence HTML</dt>\n",
    "  <dd>\n",
    "    <p>Dans notre <a href=\"/fr/docs/Web/HTML/Reference\">référence\n",
    "    exhaustive</a>, vous trouverez le détail de chaque élément et attribut\n",
    "    constituant HTML.</p>\n",
    "  </dd>\n",
    "</dl>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 1\n",
    "\n",
    "do_match = [\n",
    "    '<dt id=\"introduction_au_html\">Introduction au HTML</dt>',\n",
    "    '<dt id=\"tutoriels_html\">Tutoriels HTML</dt>',\n",
    "    '<dt id=\"référence_html\">Référence HTML</dt>'\n",
    "]\n",
    "\n",
    "dont_match = [\n",
    "    '<a href=\"/fr/docs/Web/HTML/Reference\">référence exhaustive</a>',\n",
    "    '<dd><p>Vous faites vos premiers pas dans le développement web ?'\n",
    "]\n",
    "\n",
    "regex = r'your_solution'\n",
    "\n",
    "for res in do_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')\n",
    "        \n",
    "for res in dont_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 2\n",
    "\n",
    "do_match = [\n",
    "    '<a href=\"/fr/docs/Learn/Getting_started_with_the_web/HTML_basics\">Nos articles sur les bases de HTML</a>',\n",
    "    '<a href=\"/fr/docs/Learn/HTML\">notre section Apprendre HTML</a>',\n",
    "    '<a href=\"/fr/docs/Web/HTML/Reference\">référence exhaustive</a>'\n",
    "]\n",
    "\n",
    "dont_match = [\n",
    "    '<dt id=\"référence_html\">Référence HTML</dt>',\n",
    "    '<dd><p>Vous faites vos premiers pas dans le développement web ?'\n",
    "]\n",
    "\n",
    "regex = r'your_solution'\n",
    "\n",
    "for res in do_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')\n",
    "        \n",
    "for res in dont_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 3\n",
    "\n",
    "do_match = [\n",
    "  \"\"\" \n",
    "  <dt id=\"tutoriels_html\">Tutoriels HTML</dt>\n",
    "  <dd>\n",
    "    <p>Pour plus d'informations sur l'utilisation du HTML, des tutoriels et des\n",
    "    exemples complets, vous pouvez consulter <a href=\"/fr/docs/Learn/HTML\">notre\n",
    "    section Apprendre HTML</a>.</p>\n",
    "  </dd>\n",
    "  \"\"\",\n",
    "    \n",
    "  \"\"\"<dt id=\"référence_html\">Référence HTML</dt>\n",
    "  <dd>\n",
    "    <p>Dans notre <a href=\"/fr/docs/Web/HTML/Reference\">référence\n",
    "    exhaustive</a>, vous trouverez le détail de chaque élément et attribut\n",
    "    constituant HTML.</p>\n",
    "  </dd>\n",
    "  \"\"\"\n",
    "]\n",
    "\n",
    "dont_match = [\n",
    "  \"\"\" \n",
    "  <dt id=\"introduction_au_html\">Introduction au HTML</dt>\n",
    "  <dd>\n",
    "    <p>Vous faites vos premiers pas dans le développement web ? <a\n",
    "    href=\"/fr/docs/Learn/Getting_started_with_the_web/HTML_basics\">Nos articles\n",
    "    sur les bases de HTML</a> expliquent ce qu'est HTML et comment\n",
    "    l'utiliser.</p>\n",
    "  </dd>\n",
    "  \"\"\"\n",
    "]\n",
    "\n",
    "regex = r'your_solution'\n",
    "\n",
    "for res in do_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')\n",
    "        \n",
    "for res in dont_match:\n",
    "    if re.search(regex, res):\n",
    "        print(f'it\\'s a match : {res}')\n",
    "    else:\n",
    "        print(f'no match : {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4f08b",
   "metadata": {},
   "source": [
    "#### re.findall, groupes capturant et non-capturants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = 'Mon adresse email est blabla@gmail.com et pas blublu@gmail.com'\n",
    "\n",
    "match = re.findall(r'[a-z]+@[a-z]+\\.com', text)\n",
    "print(match)\n",
    "\n",
    "match = re.findall(r'([a-z]+)@[a-z]+\\.com', text)\n",
    "print(match)\n",
    "\n",
    "match = re.findall(r'([a-z]+)@([a-z]+\\.com)', text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Les paroles de la chanson sont \"bla bla bla bla, bla bla bla\",\n",
    "c'est ma préféré. Par contre je déteste celle qui fait\n",
    "\"bla bla bla, bla bla bla bla\"\n",
    "\"\"\"\n",
    "\n",
    "match = re.findall(r'((bla,? ?)+)', text)\n",
    "print(match)\n",
    "\n",
    "match = re.findall(r'((?:bla,? ?)+)', text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95682b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = \"J'aime les pêches, mais aussi les poires\"\n",
    "\n",
    "match = re.findall(r'p(êche|oire)s', text)\n",
    "print(match)\n",
    "\n",
    "match = re.findall(r'(p(?:êche|oire)s)', text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "<dl>\n",
    "  <dt id=\"introduction_au_html\">Introduction au HTML</dt>\n",
    "  <dd>\n",
    "    <p>Vous faites vos premiers pas dans le développement web ? <a\n",
    "    href=\"/fr/docs/Learn/Getting_started_with_the_web/HTML_basics\">Nos articles\n",
    "    sur les bases de HTML</a> expliquent ce qu'est HTML et comment\n",
    "    l'utiliser.</p>\n",
    "  </dd>\n",
    "  <dt id=\"tutoriels_html\">Tutoriels HTML</dt>\n",
    "  <dd>\n",
    "    <p>Pour plus d'informations sur l'utilisation du HTML, des tutoriels et des\n",
    "    exemples complets, vous pouvez consulter <a href=\"/fr/docs/Learn/HTML\">notre\n",
    "    section Apprendre HTML</a>.</p>\n",
    "  </dd>\n",
    "  <dt id=\"référence_html\">Référence HTML</dt>\n",
    "  <dd>\n",
    "    <p>Dans notre <a href=\"/fr/docs/Web/HTML/Reference\">référence\n",
    "    exhaustive</a>, vous trouverez le détail de chaque élément et attribut\n",
    "    constituant HTML.</p>\n",
    "  </dd>\n",
    "</dl>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 1\n",
    "\n",
    "regex = 'your_solution'\n",
    "match = re.findall(regex, text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 2\n",
    "\n",
    "regex = 'your_solution'\n",
    "match = re.findall(regex, text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 3\n",
    "\n",
    "match = re.findall(regex, text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4e33c",
   "metadata": {},
   "source": [
    "#### re.sub et options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd9574",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Mon adresse email est blabla@gmail.com et pas blublu@gmail.com'\n",
    "\n",
    "new_text = re.sub(r'[a-z]+@[a-z]+\\.com', '<EMAIL>', text)\n",
    "print(new_text)\n",
    "\n",
    "new_text = re.sub(r'([a-z]+)@([a-z]+\\.com)','\\\\1[at]\\\\2', text)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f0ef1",
   "metadata": {},
   "source": [
    "#### Nettoyage et normalisation de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = \"\"\"\n",
    "The packgae has been delivered but is damaged (photo:\n",
    "https://i.postimg.cc/TPr4LQwY/MEA2-1.jpg). When i openned iy , \n",
    "every flask was broke :( I put 1 star because I cannot put \n",
    "-100 starts #wtf >>>>>>>>>>> dont buy this brand $$$$ \\U0001F643\n",
    "\"\"\"\n",
    "\n",
    "text_fr = \"\"\"\n",
    "Le colsi a été livré mais cabossé (photo :\n",
    "https://i.postimg.cc/TPr4LQwY/MEA2-1.jpg). Quan j'ai ouvert , \n",
    "tous les flacons été casser :( abusé je met 1 étoiles parce \n",
    "que je peux pas mettre -100 étoiles #labus >>>>>>>>>>> \n",
    "plus jamais cette marque $$$$ \\U0001F643\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aaefba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Bhuvana/t5-base-spellchecker\")\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(\"Bhuvana/t5-base-spellchecker\")\n",
    "\n",
    "def correct(inputs):\n",
    "    input_ids = tokenizer.encode(inputs, return_tensors = 'pt')\n",
    "    sample_output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample            = True,\n",
    "        max_length           = 50,\n",
    "        top_p                = 0.99,\n",
    "        num_return_sequences = 1\n",
    "    )\n",
    "    res = tokenizer.decode(sample_output[0], skip_special_tokens = True)\n",
    "    return res\n",
    "\n",
    "text = \"christmas is celbrated on decembr 25 evry ear\"\n",
    "\n",
    "print(correct(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59975d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct(\"When i openned iy , every flask was broke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell_en = Speller('en')\n",
    "spell_fr = Speller('fr')\n",
    "\n",
    "print(spell_en(\"When i openned iy , every flask was broke\"))\n",
    "print(spell_fr(\"Quan j'ai ouvert , tous les flacons été casser\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0febb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "print(clean(text_en,\n",
    "    fix_unicode                  = True,\n",
    "    to_ascii                     = False,\n",
    "    lower                        = True,\n",
    "    no_line_breaks               = True,\n",
    "    no_urls                      = True,\n",
    "    no_emails                    = True,\n",
    "    no_phone_numbers             = True,\n",
    "    no_numbers                   = True,\n",
    "    no_digits                    = True,\n",
    "    no_currency_symbols          = True,\n",
    "    no_punct                     = False,\n",
    "    replace_with_punct           = \"\",\n",
    "    replace_with_url             = \"<URL>\",\n",
    "    replace_with_email           = \"<EMAIL>\",\n",
    "    replace_with_phone_number    = \"<PHONE>\",\n",
    "    replace_with_number          = \"<NUMBER>\",\n",
    "    replace_with_digit           = \"<DIGIT>\",\n",
    "    replace_with_currency_symbol = \"<CUR>\",\n",
    "    lang                         = \"en\"\n",
    "))\n",
    "\n",
    "print(clean(text_fr,\n",
    "    fix_unicode                  = True,\n",
    "    to_ascii                     = False,\n",
    "    lower                        = True,\n",
    "    no_line_breaks               = True,\n",
    "    no_urls                      = True,\n",
    "    no_emails                    = True,\n",
    "    no_phone_numbers             = True,\n",
    "    no_numbers                   = True,\n",
    "    no_digits                    = True,\n",
    "    no_currency_symbols          = True,\n",
    "    no_punct                     = False,\n",
    "    replace_with_punct           = \"\",\n",
    "    replace_with_url             = \"<URL>\",\n",
    "    replace_with_email           = \"<EMAIL>\",\n",
    "    replace_with_phone_number    = \"<PHONE>\",\n",
    "    replace_with_number          = \"<NUMBER>\",\n",
    "    replace_with_digit           = \"<DIGIT>\",\n",
    "    replace_with_currency_symbol = \"<CUR>\",\n",
    "    lang                         = \"fr\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "hashtag_regex = r'#[a-zà-ÿ_]+'\n",
    "\n",
    "print(re.sub(hashtag_regex, '<hashtag>', text_en))\n",
    "print(re.sub(hashtag_regex, '<hashtag>', text_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef373ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.emo_unicode import EMOTICONS_EMO, UNICODE_EMOJI\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        emot_regex = r'(' + re.escape(emot) + ')'\n",
    "        text = re.sub(emot_regex, \"_\".join(\n",
    "            UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").replace(\"-\",\"_\").lower().split()), text\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        emot_regex = r'(' + re.escape(emot) + ')'\n",
    "        text = re.sub(emot_regex, \"_\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").lower().split()), text)\n",
    "    return text\n",
    "\n",
    "print(convert_emojis(convert_emoticons(text_en)))\n",
    "print(convert_emojis(convert_emoticons(text_fr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c728e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_char_regex = r'[^_A-zÀ-ÿ<>0-9\\'\\s.,;?!:%\\(\\ç)-+]+'\n",
    "not_tag_regex      = r'(?:<{2,}|>{2,})'\n",
    "space_norm_regex   = r'\\s+'\n",
    "\n",
    "text_en_norm = re.sub(special_char_regex, '', text_en)\n",
    "text_fr_norm = re.sub(special_char_regex, '', text_fr)\n",
    "\n",
    "text_en_norm = re.sub(not_tag_regex, '', text_en_norm)\n",
    "text_fr_norm = re.sub(not_tag_regex, '', text_fr_norm)\n",
    "\n",
    "text_en_norm = re.sub(space_norm_regex, ' ', text_en_norm).strip()\n",
    "text_fr_norm = re.sub(space_norm_regex, ' ', text_fr_norm).strip()\n",
    "\n",
    "print(text_en_norm)\n",
    "print(text_fr_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f82c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n",
    "def infer_spaces(s, lang):\n",
    "    \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "    without spaces.\"\"\"\n",
    "\n",
    "    # Find the best match for the i first characters, assuming cost has\n",
    "    # been built for the i-1 first characters.\n",
    "    # Returns a pair (match_cost, match_length).\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i-25):i]))\n",
    "        word_costs = list()\n",
    "        for k,c in candidates:\n",
    "            candidate_word = s[i-k-1:i]\n",
    "            word_freq      = word_frequency(candidate_word, lang)\n",
    "            if word_freq == 0.0:\n",
    "                word_costs.append(9e999)\n",
    "            else:\n",
    "                word_costs.append(c + (1.0 / (word_freq * len(candidate_word) + 1)))\n",
    "        \n",
    "        min_cost = min(word_costs)\n",
    "        \n",
    "        return min_cost, word_costs.index(min_cost) + 1\n",
    "\n",
    "    # Build the cost array.\n",
    "    cost = [0]\n",
    "    for i in range(1,len(s)+1):\n",
    "        c,k = best_match(i)\n",
    "        cost.append(c)\n",
    "\n",
    "    # Backtrack to recover the minimal-cost string.\n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i>0:\n",
    "        c,k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i-k:i])\n",
    "        i -= k\n",
    "\n",
    "    return \" \".join(reversed(out))\n",
    "\n",
    "str_en = 'howtosplittextwithoutspacesintolistofwords'\n",
    "str_fr = 'lesapprocheslesplusaupointdanscedomainenesontpaspubliques'\n",
    "\n",
    "print(infer_spaces(str_en, 'en'))\n",
    "print(infer_spaces(str_fr, 'fr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e62007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "print(TextBlob(text_en).correct())\n",
    "\n",
    "word = Word('falibility')\n",
    "print(word.spellcheck())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038581af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_en(text):\n",
    "    cleaned_text = clean(text,\n",
    "        fix_unicode                  = True,\n",
    "        to_ascii                     = False,\n",
    "        lower                        = True,\n",
    "        no_line_breaks               = True,\n",
    "        no_urls                      = True,\n",
    "        no_emails                    = True,\n",
    "        no_phone_numbers             = True,\n",
    "        no_numbers                   = True,\n",
    "        no_digits                    = True,\n",
    "        no_currency_symbols          = True,\n",
    "        no_punct                     = False,\n",
    "        replace_with_punct           = \"\",\n",
    "        replace_with_url             = \"<URL>\",\n",
    "        replace_with_email           = \"<EMAIL>\",\n",
    "        replace_with_phone_number    = \"<PHONE>\",\n",
    "        replace_with_number          = \"<NUMBER>\",\n",
    "        replace_with_digit           = \"<DIGIT>\",\n",
    "        replace_with_currency_symbol = \"<CUR>\",\n",
    "        lang                         = \"en\"\n",
    "    )\n",
    "    \n",
    "    cleaned_text = convert_emojis(cleaned_text)\n",
    "    cleaned_text = convert_emoticons(cleaned_text)\n",
    "    \n",
    "    hashtag_regex      = r'#[a-zà-ÿ_]+'\n",
    "    special_char_regex = r'[^_A-zÀ-ÿ<>0-9\\'\\s.,;?!:%\\(\\ç)-+]+'\n",
    "    not_tag_regex      = r'(?:<{2,}|>{2,})'\n",
    "    space_norm_regex   = r'\\s+'\n",
    "    \n",
    "    cleaned_text = re.sub(hashtag_regex, '<hashtag>', cleaned_text)\n",
    "    cleaned_text = re.sub(special_char_regex, '', cleaned_text)\n",
    "    cleaned_text = re.sub(not_tag_regex, '', cleaned_text)\n",
    "    cleaned_text = re.sub(space_norm_regex, ' ', cleaned_text).strip()\n",
    "    \n",
    "    corrected = ''\n",
    "    for word in cleaned_text.split():\n",
    "        if not re.search('[<>_:,.!?\\(\\])]', word): \n",
    "            alt_words = Word(word).spellcheck()\n",
    "            if len(alt_words) > 0:\n",
    "                alt_word, confidence = alt_words[0]\n",
    "                if confidence > 0.6:\n",
    "                    corrected += alt_word + ' '\n",
    "                    continue\n",
    "            \n",
    "        corrected += word + ' '\n",
    "        \n",
    "    cleaned_text = corrected\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "print(clean_en(text_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d31940",
   "metadata": {},
   "source": [
    "### 2.3 - Etiqueter les symboles (token tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3183d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "NASA is \"go\" for a post-breakfast launch of the biggest rocket ever—and that\n",
    "includes the final flight of the Saturn V in 1973. A flight readiness review\n",
    "this week confirmed that its Artemis-1 mission will launch during a two-hour\n",
    "window that opens at 8:33 a.m. EDT on Monday, August 29. However, if for any\n",
    "reason it doesn’t launch on schedule then the next time it can go is at\n",
    "lunchtime on Friday, September 2, 2022.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0f569",
   "metadata": {},
   "source": [
    "#### Part-Of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbea169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\tSPACE\n",
      "NASA\tPROPN\n",
      "is\tAUX\n",
      "\"\tPUNCT\n",
      "go\tVERB\n",
      "\"\tPUNCT\n",
      "for\tADP\n",
      "a\tDET\n",
      "post\tADJ\n",
      "-\tADJ\n",
      "breakfast\tADJ\n",
      "launch\tNOUN\n",
      "of\tADP\n",
      "the\tDET\n",
      "biggest\tADJ\n",
      "rocket\tNOUN\n",
      "ever\tADV\n",
      "—\tPUNCT\n",
      "and\tCCONJ\n",
      "that\tSCONJ\n",
      "\n",
      "\tSPACE\n",
      "includes\tVERB\n",
      "the\tDET\n",
      "final\tADJ\n",
      "flight\tNOUN\n",
      "of\tADP\n",
      "the\tDET\n",
      "Saturn\tPROPN\n",
      "V\tPROPN\n",
      "in\tADP\n",
      "1973\tNUM\n",
      ".\tPUNCT\n",
      "A\tDET\n",
      "flight\tNOUN\n",
      "readiness\tNOUN\n",
      "review\tNOUN\n",
      "\n",
      "\tSPACE\n",
      "this\tDET\n",
      "week\tNOUN\n",
      "confirmed\tVERB\n",
      "that\tSCONJ\n",
      "its\tPRON\n",
      "Artemis-1\tPROPN\n",
      "mission\tNOUN\n",
      "will\tAUX\n",
      "launch\tVERB\n",
      "during\tADP\n",
      "a\tDET\n",
      "two\tNUM\n",
      "-\tPUNCT\n",
      "hour\tNOUN\n",
      "\n",
      "\tSPACE\n",
      "window\tNOUN\n",
      "that\tPRON\n",
      "opens\tVERB\n",
      "at\tADP\n",
      "8:33\tNUM\n",
      "a.m.\tNOUN\n",
      "EDT\tPROPN\n",
      "on\tADP\n",
      "Monday\tPROPN\n",
      ",\tPUNCT\n",
      "August\tPROPN\n",
      "29\tNUM\n",
      ".\tPUNCT\n",
      "However\tADV\n",
      ",\tPUNCT\n",
      "if\tSCONJ\n",
      "for\tADP\n",
      "any\tDET\n",
      "\n",
      "\tSPACE\n",
      "reason\tNOUN\n",
      "it\tPRON\n",
      "does\tAUX\n",
      "n’t\tPART\n",
      "launch\tVERB\n",
      "on\tADP\n",
      "schedule\tNOUN\n",
      "then\tADV\n",
      "the\tDET\n",
      "next\tADJ\n",
      "time\tNOUN\n",
      "it\tPRON\n",
      "can\tAUX\n",
      "go\tVERB\n",
      "is\tAUX\n",
      "at\tADP\n",
      "\n",
      "\tSPACE\n",
      "lunchtime\tNOUN\n",
      "on\tADP\n",
      "Friday\tPROPN\n",
      ",\tPUNCT\n",
      "September\tPROPN\n",
      "2\tNUM\n",
      ",\tPUNCT\n",
      "2022\tNUM\n",
      ".\tPUNCT\n",
      "\n",
      "\tSPACE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text}\\t{token.pos_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text} : {token.tag_} : {token.morph}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text} => {token.dep_} => {token.head.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc.ents:\n",
    "    print(f'{entity.text} : {entity.label_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba017a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0a3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikidata entities summary ordered by categories\n",
      "calendar month (2) : August,September\n",
      "month of the Gregorian calendar (2) : August,September\n",
      "space agency (1) : National Aeronautics and Space Administration\n",
      "architectural structure (1) : slipway\n",
      "vehicle (1) : rocket\n",
      "disposable product (1) : rocket\n",
      "flying machine (1) : rocket\n",
      "novel (1) : Final Flight\n",
      "Saturn (1) : Saturn V\n",
      "air force unit (1) : flight\n",
      "\n",
      "wikidata entities details\n",
      "sentence 0\n",
      "  entity 0\n",
      "    text: NASA\n",
      "    id: 23548\n",
      "    label: National Aeronautics and Space Administration\n",
      "    description: space-related agency of the United States government\n",
      "  entity 1\n",
      "    text: launch\n",
      "    id: 361945\n",
      "    label: slipway\n",
      "    description: ramp on the shore by which ships or boats can be moved to and from the water\n",
      "  entity 2\n",
      "    text: rocket\n",
      "    id: 41291\n",
      "    label: rocket\n",
      "    description: pyrokinetic engine used for propulsion; for the incendiary weapon, see Q2037215\n",
      "  entity 3\n",
      "    text: final flight\n",
      "    id: 48782185\n",
      "    label: Final Flight\n",
      "    description: 1986 novel written by Stephen Coonts\n",
      "  entity 4\n",
      "    text: Saturn V\n",
      "    id: 54363\n",
      "    label: Saturn V\n",
      "    description: American human-rated expendable rocket\n",
      "sentence 1\n",
      "  entity 0\n",
      "    text: flight\n",
      "    id: 13583449\n",
      "    label: flight\n",
      "    description: military unit of about three to six aircraft or an equivalent-sized aviation-related element\n",
      "  entity 1\n",
      "    text: week\n",
      "    id: 23387\n",
      "    label: week\n",
      "    description: unit of time\n",
      "  entity 2\n",
      "    text: mission\n",
      "    id: 373069\n",
      "    label: evangelism\n",
      "    description: spreading the Gospel of Jesus Christ for the purpose of conversion or a rapprochement with Christianity\n",
      "  entity 3\n",
      "    text: window\n",
      "    id: 35473\n",
      "    label: window\n",
      "    description: transparent or translucent opening\n",
      "  entity 4\n",
      "    text: EDT\n",
      "    id: 941023\n",
      "    label: Eastern Time Zone\n",
      "    description: time zone observing UTC−05:00 during standard time and UTC−04:00 during daylight saving time\n",
      "  entity 5\n",
      "    text: Monday\n",
      "    id: 105\n",
      "    label: Monday\n",
      "    description: day of the week\n",
      "  entity 6\n",
      "    text: August\n",
      "    id: 122\n",
      "    label: August\n",
      "    description: eighth month in the Julian and Gregorian calendars\n",
      "sentence 2\n",
      "  entity 0\n",
      "    text: reason\n",
      "    id: 178354\n",
      "    label: reason\n",
      "    description: capacity for consciously making sense of things\n",
      "  entity 1\n",
      "    text: schedule\n",
      "    id: 1123036\n",
      "    label: scheduler\n",
      "    description: in computing, what carries out the scheduling activity\n",
      "  entity 2\n",
      "    text: next time\n",
      "    id: 923783\n",
      "    label: Next Time\n",
      "    description: \n",
      "  entity 3\n",
      "    text: lunchtime\n",
      "    id: 28094676\n",
      "    label: Lunchtime\n",
      "    description: Racehorse\n",
      "  entity 4\n",
      "    text: Friday\n",
      "    id: 673486\n",
      "    label: Friday\n",
      "    description: 1995 film directed by F. Gary Gray\n",
      "  entity 5\n",
      "    text: September\n",
      "    id: 123\n",
      "    label: September\n",
      "    description: ninth month in the Julian and Gregorian calendars\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(\"entityLinker\", last = True)\n",
    "doc = nlp(text)\n",
    "\n",
    "print('wikidata entities summary ordered by categories')\n",
    "doc._.linkedEntities.print_super_entities()\n",
    "\n",
    "print()\n",
    "print('wikidata entities details')\n",
    "for s_index, sentence in enumerate(doc.sents):\n",
    "    entities = sentence._.linkedEntities\n",
    "    print(f'sentence {s_index}')\n",
    "    for e_index, entity in enumerate(entities):\n",
    "        print(f'  entity {e_index}')\n",
    "        print(f'    text: {entity.get_span()}')\n",
    "        print(f'    id: {entity.get_id()}')\n",
    "        print(f'    label: {entity.get_label()}')\n",
    "        print(f'    description: {entity.get_description()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c75777",
   "metadata": {},
   "source": [
    "## Partie 3 -Vectorisation de symboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = \"\"\"\n",
    "Il jouait on ne sait quel effrayant jeu de cache-cache avec la mort ; \n",
    "chaque fois que la face camarde du spectre s'approchait, le gamin \n",
    "lui donnait une pichenette. Une balle pourtant, mieux ajustée ou \n",
    "plus traître que les autres, finit par atteindre l'enfant feu follet. \n",
    "On vit Gavroche chanceler, puis il s'affaissa.\n",
    "\"\"\"\n",
    "\n",
    "vocab       = dict()\n",
    "vocab_index = 0\n",
    "for word in text.split():\n",
    "    if word not in vocab:\n",
    "        vocab[word]  = vocab_index\n",
    "        vocab_index += 1\n",
    "        \n",
    "def make_one_hot(word, vocab):\n",
    "    vect = np.zeros(len(vocab))\n",
    "    if word in vocab:\n",
    "        vect[vocab[word]] = 1\n",
    "    \n",
    "    return vect\n",
    "    \n",
    "print(make_one_hot('Gavroche', vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ac8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'chien', 'chat', 'ours', 'loup', \n",
    "    'chat', 'chat', 'ours', 'chien', \n",
    "    'chien', 'loup', 'ours', 'ours'\n",
    "]\n",
    "\n",
    "classes     = dict()\n",
    "class_index = 0\n",
    "for label in labels:\n",
    "    if label not in classes:\n",
    "        classes[label] = class_index\n",
    "        class_index  += 1\n",
    "\n",
    "def make_one_hot(label, classes):\n",
    "    vect = np.zeros(len(classes))\n",
    "    if label in classes:\n",
    "        vect[classes[label]] = 1\n",
    "    \n",
    "    return list(vect)\n",
    "\n",
    "one_hot_labels = list()\n",
    "for label in labels:\n",
    "    one_hot_labels.append(make_one_hot(label, classes))\n",
    "    \n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8315663",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = [\n",
    "    [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], \n",
    "    [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0], \n",
    "    [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], \n",
    "    [0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], \n",
    "    [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], \n",
    "    [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e93873",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab       = dict()\n",
    "vocab_index = 0\n",
    "for letter in text.lower():\n",
    "    if letter not in vocab:\n",
    "        vocab[letter]  = vocab_index\n",
    "        vocab_index += 1\n",
    "\n",
    "def vectorize_with_freq(text, vocab):\n",
    "    vect = np.zeros(len(vocab))\n",
    "    \n",
    "    for letter in text.lower():\n",
    "        vect[vocab[letter]] += 1\n",
    "        \n",
    "    vect /= len(text)\n",
    "    return vect\n",
    "\n",
    "print(vocab)\n",
    "print(vectorize_with_freq(text, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_split(text, n = 2):\n",
    "    sequence = list()\n",
    "    for i, c in enumerate(text):\n",
    "        if i + n - 1 == len(text):\n",
    "            break\n",
    "            \n",
    "        gram = c\n",
    "        for j in range(1, n):\n",
    "            gram += text[i + j]\n",
    "            \n",
    "        sequence.append(gram)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "print(n_gram_split(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "                \n",
    "min_freq = 9e999\n",
    "for word in text.split():\n",
    "    if word in vocab:\n",
    "        word_freq = word_frequency(word, 'fr')\n",
    "        if word_freq != 0.0:\n",
    "            if word_freq < min_freq:\n",
    "                min_freq = word_freq\n",
    "                \n",
    "def tf_idf(text, vocab, min_freq):\n",
    "    vect = np.zeros(len(vocab))\n",
    "    \n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        vect[vocab[word]] += 1\n",
    "        \n",
    "    vect /= len(words)\n",
    "    \n",
    "    for word, index in vocab.items():\n",
    "        word_freq = word_frequency(\n",
    "            word    = word, \n",
    "            lang    = 'fr', \n",
    "            minimum = min_freq\n",
    "        )\n",
    "        vect[index] /= word_freq\n",
    "        \n",
    "    return vect\n",
    "\n",
    "print(tf_idf(text, vocab, min_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f505dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'un chat a un collier',\n",
    "    'un chien n\\'aiment pas un chat',\n",
    "    'un chien a une laisse',\n",
    "    'elle a un chien et un chat'\n",
    "]\n",
    "\n",
    "vocab = {\n",
    "    'chien'   : 0,\n",
    "    'chat'    : 1,\n",
    "    'collier' : 2,\n",
    "    'laisse'  : 3\n",
    "}\n",
    "            \n",
    "word_vector_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "for document in documents:\n",
    "    words = document.split()\n",
    "    for i in range(len(words)):\n",
    "        w_1 = words[i]\n",
    "        if w_1 not in vocab:\n",
    "            continue\n",
    "            \n",
    "        w_1_index = vocab[w_1]\n",
    "        for j in range(len(words)):\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            w_2 = words[j]\n",
    "            if w_2 not in vocab:\n",
    "                continue\n",
    "                \n",
    "            w_2_index = vocab[w_2]\n",
    "            \n",
    "            word_vector_matrix[w_1_index][w_2_index] += 1\n",
    "    \n",
    "print(word_vector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "word_freq          = np.zeros(len(vocab))\n",
    "word_vector_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "                              \n",
    "for document in documents:\n",
    "    words = document.split()\n",
    "    for i in range(len(words)):\n",
    "        w_1 = words[i]\n",
    "        if w_1 not in vocab:\n",
    "            continue\n",
    "            \n",
    "        w_1_index = vocab[w_1]\n",
    "        word_freq[w_1_index] += 1\n",
    "        for j in range(len(words)):\n",
    "            if i == j:\n",
    "                continue\n",
    "                \n",
    "            w_2 = words[j]\n",
    "            if w_2 not in vocab:\n",
    "                continue\n",
    "                \n",
    "            w_2_index = vocab[w_2]\n",
    "            \n",
    "            word_vector_matrix[w_1_index][w_2_index] += 1\n",
    "\n",
    "word_freq          /= len(vocab)\n",
    "word_vector_matrix /= len(vocab)\n",
    "                              \n",
    "for i in range(len(word_freq)):\n",
    "    for j in range(len(word_freq)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        if word_vector_matrix[i][j] != 0:\n",
    "            word_vector_matrix[i][j] /= word_freq[i] * word_freq[j]\n",
    "            word_vector_matrix[i][j]  = math.log(word_vector_matrix[i][j])\n",
    "                     \n",
    "print(word_vector_matrix)\n",
    "print()\n",
    "print(vocab)\n",
    "for i, word in enumerate(vocab):\n",
    "    word_vector = word_vector_matrix[i]\n",
    "    for j, v in enumerate(word_vector):\n",
    "        if v < 0:\n",
    "            word_vector[j] = 0\n",
    "            \n",
    "    print(f'{word} : {word_vector}')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9305b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# pre-trained skip-gram\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "print(f'word #10/{len(model.index_to_key)} is \"{model.index_to_key[10]}\"')\n",
    "print(f'word vector for \"was\" is {model[\"was\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('cherry', 'pear'),\n",
    "    ('cherry', 'apple'),   \n",
    "    ('cherry', 'cereal'),  \n",
    "    ('cherry', 'cake'),    \n",
    "    ('cherry', 'communism'),\n",
    "]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    print(f'sim({w1}, {w2}) = {model.similarity(w1, w2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656013ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar('people', topn = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive = ['woman', 'king'], negative = ['man'], topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f693c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_list = [\n",
    "    'blue', 'red', 'orange', 'green', \n",
    "    'eagle', 'bear', 'cat', 'dog', \n",
    "    'computer', 'keyboard', 'internet', 'hacking'\n",
    "]\n",
    "X     = model[word_list]\n",
    "#X_pca = PCA().fit_transform(X)[:,:2]\n",
    "\n",
    "X_tsne = TSNE(\n",
    "    n_components  = 2, \n",
    "    learning_rate = 'auto',\n",
    "    #init          = 'random', \n",
    "    perplexity    = 3\n",
    ").fit_transform(X)\n",
    "\n",
    "plt.figure(figsize = (7, 5)) \n",
    "for i in range(X_tsne.shape[0]):\n",
    "    x, y = X_tsne[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        word_list[i],\n",
    "        xy         = (x, y),\n",
    "        xytext     = (5, 2),\n",
    "        textcoords = 'offset points',\n",
    "        ha         = 'right',\n",
    "        va         = 'bottom'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf846b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "639d6d89",
   "metadata": {},
   "source": [
    "## Partie 4 - Deep Learning pour NLP\n",
    "\n",
    "### Partie 4.2 - Perceptron multicouches (MLP)\n",
    "\n",
    "#### Neurone formel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def h(x):\n",
    "  if x > 0:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def neuron_train(X, Y, w, µ):\n",
    "  it      = 0\n",
    "  is_over = False\n",
    "  while is_over is False:\n",
    "    is_over = True\n",
    "\n",
    "    for x, y_ex in zip(X, Y):\n",
    "      a = np.inner(w, x) # vector product\n",
    "      y = h(a)\n",
    "      e = y_ex - y\n",
    "\n",
    "      w = w + µ * e * x\n",
    "\n",
    "      if e != 0:\n",
    "        is_over = False\n",
    "\n",
    "    it += 1\n",
    "\n",
    "  return w, it\n",
    "\n",
    "w = np.array([0.3, 0.1, -0.2])\n",
    "X = np.array([ # we add 1 to represent the bias\n",
    "  [1, 0, 0],\n",
    "  [1, 1, 0],\n",
    "  [1, 0, 1],\n",
    "  [1, 1, 1]\n",
    "])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "µ = 0.5\n",
    "\n",
    "w, it = neuron_train(X, y, w, µ)\n",
    "print(w, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_predict(X, w):\n",
    "  Y = list()\n",
    "  for x in X:\n",
    "    a = np.inner(w, x)\n",
    "    y = h(a)\n",
    "    Y.append(y)\n",
    "\n",
    "  return Y\n",
    "\n",
    "print(neuron_predict(X, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b97eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.3, 0.1, -0.2])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "w, it = neuron_train(X, y, w, µ)\n",
    "\n",
    "print(w, it)\n",
    "print(neuron_predict(X, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945878ba",
   "metadata": {},
   "source": [
    "#### Implémentation d'un MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4dcad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import math\n",
    "\n",
    "def generate_dataset(n_samples):\n",
    "  # nous générons des vecteurs de taille 2 dont les valeurs sont entre -1 et 1\n",
    "  x = torch.rand(n_samples, 2) * 2 - 1\n",
    "\n",
    "  # nous calculons la distance entre le point et l'origine\n",
    "  x_norms = (x ** 2).sum(axis = 1).sqrt()\n",
    "\n",
    "  # le rayon du disque\n",
    "  radius = math.sqrt(2 / math.pi)\n",
    "\n",
    "  # compare la norme au rayon du disque, créé un masque booléen,\n",
    "  # puis trasnforme en entier 8 octects.\n",
    "  y = (x_norms < radius).long()\n",
    "\n",
    "  return TensorDataset(x, y)\n",
    "  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MLP, self).__init__()\n",
    "    self.hidden_layer_1    = nn.Linear(2, 64)\n",
    "    self.hidden_layer_2    = nn.Linear(64, 64)\n",
    "    self.output_layer      = nn.Linear(64, 2)\n",
    "    self.hidden_transfer_1 = nn.ReLU()\n",
    "    self.hidden_transfer_2 = nn.ReLU()\n",
    "    self.output_transfer   = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.hidden_layer_1(x)\n",
    "    x = self.hidden_transfer_1(x)\n",
    "    x = self.hidden_layer_2(x)\n",
    "    x = self.hidden_transfer_2(x)\n",
    "    x = self.output_layer(x)\n",
    "    x = self.output_transfer(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def evaluate(model, dataset, batch_size):\n",
    "  dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "  correct_pred = 0\n",
    "  total_pred   = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      y_pred        = model(X)\n",
    "      y_pred_class  = y_pred.argmax(dim = 1)\n",
    "      correct_pred += (y_pred_class == y).sum().item()\n",
    "      total_pred   += len(y)\n",
    "\n",
    "  return correct_pred / total_pred\n",
    "\n",
    "def train(\n",
    "  model, loss_func, optimizer, train_dataset,\n",
    "  test_dataset, epochs, batch_size\n",
    "):\n",
    "  dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for X, y in dataloader:\n",
    "      y_pred = model(X)\n",
    "      loss   = loss_func(y_pred, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      train_accuracy = evaluate(model, train_dataset, batch_size)\n",
    "      test_accuracy  = evaluate(model, test_dataset, batch_size)\n",
    "      print(\n",
    "        f'{epoch:3} -> {100 * train_accuracy:5.3f}% train accuracy',\n",
    "        f'{epoch:3} -> {100 * test_accuracy:5.3f}% test accuracy'\n",
    "      )\n",
    "\n",
    "n_samples     = 3000\n",
    "epochs        = 300\n",
    "batch_size    = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_dataset = generate_dataset(n_samples)\n",
    "test_dataset  = generate_dataset(n_samples)\n",
    "model         = MLP()\n",
    "loss_func     = nn.NLLLoss()\n",
    "optimizer     = optim.SGD(\n",
    "  params = model.parameters(),\n",
    "  lr     = learning_rate\n",
    ")\n",
    "train(\n",
    "  model, loss_func, optimizer, train_dataset,\n",
    "  test_dataset, epochs, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa62c44",
   "metadata": {},
   "source": [
    "#### MLP Exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2431e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "df_train = load_dataset('amazon_reviews_multi', split = 'train').to_pandas()\n",
    "vocab       = dict() \n",
    "langs       = dict()\n",
    "vocab_index = 0\n",
    "langs_index = 0\n",
    "\n",
    "# sélection de 10000 échantillons\n",
    "samples = random.sample(list(df.iterrows()), 10000)\n",
    "\n",
    "# construction du vocabulaire\n",
    "for _, row in samples:\n",
    "    lang   = row['language']\n",
    "    review = row['review_body']\n",
    "    \n",
    "    if lang not in langs:\n",
    "        langs[lang]  = langs_index\n",
    "        langs_index += 1\n",
    "        \n",
    "    for c in review:\n",
    "        if c not in vocab:\n",
    "            vocab[c]     = vocab_index\n",
    "            vocab_index += 1\n",
    "        \n",
    "# construction du dataset\n",
    "X           = list()\n",
    "y           = list()\n",
    "feature_len = len(vocab)\n",
    "label_len   = len(langs)\n",
    "\n",
    "for _, row in samples:\n",
    "    lang   = row['language']\n",
    "    review = row['review_body']\n",
    "    x      = [0.0] * feature_len\n",
    "    \n",
    "    for c in review:\n",
    "        x[vocab[c]] += 1.0\n",
    "        \n",
    "    for i, _ in enumerate(x):\n",
    "        x[i] /= len(review)\n",
    "    \n",
    "    X.append(x)\n",
    "    y.append(langs[lang])\n",
    "\n",
    "# création des dataloaders pytorch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array([np.array(x) for x in X])\n",
    "y = np.array([np.array(y_ex) for y_ex in y])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.as_tensor(X_train, dtype = torch.float), \n",
    "    torch.as_tensor(y_train, dtype = torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.as_tensor(X_test, dtype = torch.float), \n",
    "    torch.as_tensor(y_test, dtype = torch.long)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from bpemb import BPEmb\n",
    "\n",
    "df_train = load_dataset('amazon_reviews_multi', split = 'train').to_pandas()\n",
    "df_test  = load_dataset('amazon_reviews_multi', split = 'test').to_pandas()\n",
    "\n",
    "# embeddings multilangage type GloVe \n",
    "# entraîné sur la co-occurrences de sous-mots\n",
    "multibpemb = BPEmb(lang = 'multi', vs = 1000000, dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cade361",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(df_train['language'].unique())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_train[df_train['language'] == 'fr']['review_body'].iloc[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_text = multibpemb.encode(text)\n",
    "embeddings     = multibpemb.embed(text)\n",
    "\n",
    "print(segmented_text)\n",
    "print(len(segmented_text))\n",
    "print(embeddings.shape)\n",
    "print(embeddings.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1360338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "padding_emb = np.zeros((1, 300))\n",
    "\n",
    "# vu la taille du dataset, nous vectoriserons les données\n",
    "# à la volée (au dernier moment)\n",
    "def vectorize(text, length = 100):\n",
    "    embeddings = multibpemb.embed(text)\n",
    "    if len(embeddings) > length:\n",
    "        embeddings = embeddings[:length, :]\n",
    "    elif len(embeddings) < length:\n",
    "        padding    = np.repeat(padding_emb, length - len(embeddings), axis = 0)\n",
    "        embeddings = np.concatenate((embeddings, padding), axis = 0)\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "vectorize(text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(\n",
    "    ['review_id', 'product_id', 'review_title', \n",
    "     'product_category', 'reviewer_id', 'language'], \n",
    "    axis = 1, inplace = True\n",
    ")\n",
    "df_test.drop(\n",
    "    ['review_id', 'product_id', 'review_title', \n",
    "     'product_category', 'reviewer_id', 'language'], \n",
    "    axis = 1, inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stars, text = df_train.iloc[5]\n",
    "print(stars)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c57966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        stars, text = df_train.iloc[idx]\n",
    "        x = np.float32(vectorize(text))\n",
    "        y = np.array([stars], dtype = 'float32')\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    AmazonDataset(df_train), batch_size = 64, \n",
    "    num_workers = 4, shuffle = True, drop_last = True\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    AmazonDataset(df_test), batch_size = 64, \n",
    "    num_workers = 4, shuffle = True, drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLPRegression(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(MLPRegression, self).__init__()\n",
    "        self.hidden_layer_1    = nn.Linear(emb_size, hidden_size)\n",
    "        self.hidden_layer_2    = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer      = nn.Linear(hidden_size, 1)\n",
    "        self.hidden_transfer_1 = nn.ReLU()\n",
    "        self.hidden_transfer_2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # les embeddings des tokens du texte sont moyennés\n",
    "        x = x.mean(dim = 1) \n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.hidden_transfer_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.hidden_transfer_2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b007bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    loss_func = nn.L1Loss()\n",
    "    score     = 0\n",
    "    count     = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            y_pred  = model(X)\n",
    "            score  += loss_func(y, y_pred).item()\n",
    "            count  += 1\n",
    "        \n",
    "    return score / count\n",
    "\n",
    "def train(\n",
    "  model, loss_func, optimizer, train_loader,\n",
    "  test_loader, epochs\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        score = 0\n",
    "        count = 0\n",
    "        for X, y in train_loader:\n",
    "            y_pred = model(X)\n",
    "            loss   = loss_func(y_pred, y)\n",
    "            score += loss.detach().item()\n",
    "            count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        test_score = evaluate(model, test_loader)\n",
    "        print(\n",
    "            f'epoch {epoch:3} -> {score / count:5.3f} train L1 Loss',\n",
    "            f'{epoch:3} -> {test_score:5.3f} test L1 Loss'\n",
    "        )\n",
    "                \n",
    "\n",
    "epochs    = 5\n",
    "model     = MLPRegression(emb_size = 300, hidden_size = 100)\n",
    "loss_func = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "train(\n",
    "    model, loss_func, optimizer, \n",
    "    train_loader, test_loader, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNRegression(nn.Module):\n",
    "    def __init__(\n",
    "        self, seq_len = 100, filters = 300, \n",
    "        window_size = 5, stride = 2, emb_len = 300,\n",
    "        hidden_size = 100\n",
    "    ):\n",
    "        super(CNNRegression, self).__init__()\n",
    "        \n",
    "        self.seq_len     = seq_len\n",
    "        self.filters     = filters\n",
    "        self.kernel_size = window_size\n",
    "        self.stride      = stride\n",
    "        self.emb_len     = emb_len\n",
    "    \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels  = emb_len, \n",
    "            out_channels = filters, \n",
    "            kernel_size  = self.kernel_size, \n",
    "            stride       = stride\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(\n",
    "            kernel_size = 4, \n",
    "            stride      = 4\n",
    "        )\n",
    "        \n",
    "        self.conv_transfer   = nn.ReLU()\n",
    "        self.hidden_layer    = nn.Linear(self.conv_output_count(), hidden_size)\n",
    "        self.output_layer    = nn.Linear(hidden_size, 1)\n",
    "        self.hidden_transfer = nn.ReLU()\n",
    "    \n",
    "    def conv_output_count(self):\n",
    "        out_conv = ((self.seq_len - 1 * (self.kernel_size - 1) - 1) / self.stride) + 1\n",
    "        out_conv = math.floor(out_conv)\n",
    "        out_pool = ((out_conv - 4) / 4) + 1\n",
    "        out_pool = math.floor(out_pool)\n",
    "        \n",
    "        return out_pool * self.filters\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv_transfer(x)\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_transfer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "epochs    = 10\n",
    "model     = CNNRegression()\n",
    "loss_func = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "train(\n",
    "    model, loss_func, optimizer, \n",
    "    train_loader, test_loader, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1675e",
   "metadata": {},
   "source": [
    "### CNN - Sentiment detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_files = [\n",
    "    './aclImdb_v1/aclImdb/train/pos/0_9.txt',\n",
    "    './aclImdb_v1/aclImdb/train/pos/115_10.txt',\n",
    "    './aclImdb_v1/aclImdb/train/pos/161_8.txt',\n",
    "    './aclImdb_v1/aclImdb/train/pos/4002_8.txt',\n",
    "    './aclImdb_v1/aclImdb/train/pos/3940_9.txt',\n",
    "    './aclImdb_v1/aclImdb/train/pos/4888_8.txt',\n",
    "    './aclImdb_v1/aclImdb/train/pos/10736_10.txt',\n",
    "    './aclImdb_v1/aclImdb/train/pos/10696_7.txt'\n",
    "]\n",
    "\n",
    "for filepath in some_files:\n",
    "    with open(filepath, 'r') as fs:\n",
    "        comment = fs.read()\n",
    "        print(comment)\n",
    "        print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab328017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    composed_word_regex = r'-'\n",
    "    br_regex            = r'<[^<>\\/]+\\/>'\n",
    "    anon_regex          = r'\\.{4,}'\n",
    "    normalise_point     = r'\\.'\n",
    "    space_regex         = r'\\s+'\n",
    "    normalise_tags      = r'><'\n",
    "    unslash             = r'\\/'\n",
    "    \n",
    "    cleaned_text = re.sub(composed_word_regex, ' ', text)\n",
    "    cleaned_text = re.sub(anon_regex, '<ANON>', cleaned_text)\n",
    "    cleaned_text = re.sub(br_regex, '', cleaned_text)\n",
    "    cleaned_text = re.sub(normalise_point, '. ', cleaned_text)\n",
    "    cleaned_text = re.sub(unslash, ' ', cleaned_text)\n",
    "    \n",
    "    cleaned_text = clean(cleaned_text,\n",
    "        fix_unicode                  = True,\n",
    "        to_ascii                     = True,\n",
    "        lower                        = True,\n",
    "        no_line_breaks               = True,\n",
    "        no_urls                      = True,\n",
    "        no_emails                    = True,\n",
    "        no_phone_numbers             = True,\n",
    "        no_numbers                   = True,\n",
    "        no_digits                    = True,\n",
    "        no_currency_symbols          = True,\n",
    "        no_punct                     = True,\n",
    "        replace_with_punct           = \"\",\n",
    "        replace_with_url             = \"<URL>\",\n",
    "        replace_with_email           = \"<EMAIL>\",\n",
    "        replace_with_phone_number    = \"<PHONE>\",\n",
    "        replace_with_number          = \"<NUMBER>\",\n",
    "        replace_with_digit           = \"<DIGIT>\",\n",
    "        replace_with_currency_symbol = \"<CUR>\",\n",
    "        lang                         = \"en\"\n",
    "    )\n",
    "    \n",
    "    cleaned_text = re.sub(space_regex, ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(normalise_tags, ' ', cleaned_text)\n",
    "    \n",
    "    cleaned = ''\n",
    "    for word in cleaned_text.split():\n",
    "        if not word in en_stopwords:\n",
    "            cleaned += word + ' '\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "for filepath in some_files:\n",
    "    with open(filepath, 'r') as fs:\n",
    "        comment = fs.read()\n",
    "        print(clean_text(comment))\n",
    "        print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gensim_api\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "\n",
    "embedding_model = gensim_api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer   = TweetTokenizer()\n",
    "padding_emb = np.zeros((1, 300))\n",
    "\n",
    "def get_vector(word):\n",
    "    try:\n",
    "        return embedding_model.get_vector(word)\n",
    "    except KeyError:\n",
    "        return np.zeros(300)\n",
    "\n",
    "def process_file(filepath, seq_len):\n",
    "    with open(filepath, 'r') as fs:\n",
    "        comment = fs.read()\n",
    "        \n",
    "    embeddings = list()\n",
    "    comment    = clean_text(comment)\n",
    "    tokens     = tokenizer.tokenize(comment)\n",
    "    if len(tokens) > seq_len:\n",
    "        tokens = tokens[:seq_len]\n",
    "        \n",
    "    embeddings = np.array([get_vector(token) for token in tokens])\n",
    "    if len(embeddings) < seq_len:\n",
    "        padding    = np.repeat(padding_emb, seq_len - len(embeddings), axis = 0)\n",
    "        embeddings = np.concatenate((embeddings, padding), axis = 0)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def process_dir(dirpath, seq_len):\n",
    "    files = list()\n",
    "    for file in listdir(dirpath):\n",
    "        if isfile(join(dirpath, file)):\n",
    "            files.append(join(dirpath, file))\n",
    "        \n",
    "    samples = [process_file(path, seq_len) for path in files]\n",
    "    samples = np.array(samples)\n",
    "    if 'pos' in dirpath:\n",
    "        labels = np.ones(len(samples))\n",
    "    else:\n",
    "        labels = np.zeros(len(samples))\n",
    "        \n",
    "    return samples, labels\n",
    "\n",
    "def make_dataset(part = 'train', seq_len = 100):\n",
    "    pos_samples, pos_labels = process_dir(\n",
    "        f'./aclImdb_v1/aclImdb/{part}/pos', \n",
    "        seq_len\n",
    "    )\n",
    "    neg_samples, neg_labels = process_dir(\n",
    "        f'./aclImdb_v1/aclImdb/{part}/neg', \n",
    "        seq_len\n",
    "    )\n",
    "\n",
    "    x = np.concatenate((pos_samples, neg_samples), axis = 0)\n",
    "    y = np.concatenate((pos_labels, neg_labels))\n",
    "    return TensorDataset(\n",
    "        torch.as_tensor(x, dtype = torch.float), \n",
    "        torch.as_tensor(y, dtype = torch.long)\n",
    "    )\n",
    "\n",
    "if isfile('sentiment_dataset.pkl'):\n",
    "    with open('sentiment_dataset.pkl', 'rb') as fs:\n",
    "        train_dataset, test_dataset = pickle.load(fs)\n",
    "else:\n",
    "    train_dataset = make_dataset('train')\n",
    "    test_dataset  = make_dataset('test')\n",
    "    \n",
    "    with open('sentiment_dataset.pkl', 'wb') as fs:\n",
    "        pickle.dump([train_dataset, test_dataset], fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28651bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size = 64, \n",
    "    num_workers = 4, shuffle = True, drop_last = True\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    test_dataset, batch_size = 64, \n",
    "    num_workers = 4, shuffle = True, drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7baee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, \n",
    "        kernel_size, stride, seq_len, pool_k\n",
    "    ):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        \n",
    "        self.seq_len     = seq_len\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride      = stride\n",
    "        self.transfer    = nn.ReLU()\n",
    "        self.pool_k      = pool_k\n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels  = in_channels, \n",
    "            out_channels = out_channels, \n",
    "            kernel_size  = kernel_size, \n",
    "            stride       = stride\n",
    "        )\n",
    "        self.pool = nn.MaxPool1d(\n",
    "            kernel_size = pool_k, \n",
    "            stride      = pool_k\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def output_seq_len(self):\n",
    "        out_conv = ((self.seq_len - self.kernel_size) / self.stride) + 1\n",
    "        out_conv = math.floor(out_conv)\n",
    "        out_pool = ((out_conv - self.pool_k) / self.pool_k) + 1\n",
    "        out_pool = math.floor(out_pool)\n",
    "        \n",
    "        return out_pool\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.transfer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, seq_len = 100, filters = 50, emb_len = 300,\n",
    "        hidden_size = 100, output_size = 2\n",
    "    ):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv_1 = ConvLayer(\n",
    "            in_channels  = emb_len, \n",
    "            out_channels = filters,\n",
    "            kernel_size  = 5,\n",
    "            stride       = 1,\n",
    "            seq_len      = seq_len,\n",
    "            pool_k       = 4\n",
    "        )\n",
    "        self.conv_2 = ConvLayer(\n",
    "            in_channels  = filters, \n",
    "            out_channels = filters,\n",
    "            kernel_size  = 4,\n",
    "            stride       = 1,\n",
    "            seq_len      = self.conv_1.output_seq_len(),\n",
    "            pool_k       = 2\n",
    "        )\n",
    "        self.conv_3 = ConvLayer(\n",
    "            in_channels  = filters, \n",
    "            out_channels = filters,\n",
    "            kernel_size  = 3,\n",
    "            stride       = 1,\n",
    "            seq_len      = self.conv_2.output_seq_len(),\n",
    "            pool_k       = 2\n",
    "        )\n",
    "        \n",
    "        mlp_feature_count    = self.conv_3.output_seq_len() * filters\n",
    "        self.hidden_layer    = nn.Linear(mlp_feature_count, hidden_size)\n",
    "        self.output_layer    = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_transfer = nn.ReLU()\n",
    "        self.output_transfer = nn.LogSoftmax(dim = 1)\n",
    "        self.dropout_1       = nn.Dropout(0.8)\n",
    "        self.dropout_2       = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_transfer(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.output_transfer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    correct_pred = 0\n",
    "    total_pred   = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            y_pred        = model(X)\n",
    "            y_pred_class  = y_pred.argmax(dim = 1)\n",
    "            correct_pred += (y_pred_class == y).sum().item()\n",
    "            total_pred   += len(y)\n",
    "\n",
    "    return correct_pred / total_pred\n",
    "\n",
    "def train(\n",
    "  model, loss_func, optimizer, train_loader,\n",
    "  test_loader, epochs\n",
    "):\n",
    "    correct_pred = 0\n",
    "    total_pred   = 0\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_loader:\n",
    "            y_pred = model(X)\n",
    "            loss   = loss_func(y_pred, y)\n",
    "            \n",
    "            y_pred_class  = y_pred.detach().argmax(dim = 1)\n",
    "            correct_pred += (y_pred_class == y).sum().item()\n",
    "            total_pred   += len(y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_accuracy = correct_pred / total_pred\n",
    "        test_accuracy  = evaluate(model, test_loader)\n",
    "        print(\n",
    "            f'{epoch:3} -> {100 * train_accuracy:5.3f}% train accuracy',\n",
    "            f'{epoch:3} -> {100 * test_accuracy:5.3f}% test accuracy'\n",
    "        )\n",
    "\n",
    "#epochs    = 10\n",
    "#model     = CNN(filters = 300)\n",
    "#loss_func = nn.NLLLoss()\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#train(\n",
    "#  model, loss_func, optimizer, \n",
    "#  train_loader, test_loader, \n",
    "#  epochs\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb1999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 -> 58.141% train accuracy   0 -> 59.046% test accuracy\n",
      "  1 -> 60.196% train accuracy   1 -> 72.700% test accuracy\n",
      "  2 -> 57.569% train accuracy   2 -> 49.984% test accuracy\n",
      "  3 -> 56.333% train accuracy   3 -> 56.138% test accuracy\n",
      "  4 -> 56.581% train accuracy   4 -> 49.271% test accuracy\n",
      "  5 -> 56.023% train accuracy   5 -> 56.546% test accuracy\n",
      "  6 -> 55.734% train accuracy   6 -> 54.948% test accuracy\n",
      "  7 -> 55.774% train accuracy   7 -> 51.542% test accuracy\n",
      "  8 -> 55.462% train accuracy   8 -> 51.931% test accuracy\n",
      "  9 -> 55.183% train accuracy   9 -> 56.370% test accuracy\n",
      " 10 -> 55.187% train accuracy  10 -> 54.884% test accuracy\n",
      " 11 -> 55.471% train accuracy  11 -> 60.581% test accuracy\n",
      " 12 -> 55.892% train accuracy  12 -> 55.990% test accuracy\n",
      " 13 -> 56.442% train accuracy  13 -> 72.083% test accuracy\n",
      " 14 -> 57.357% train accuracy  14 -> 51.254% test accuracy\n",
      " 15 -> 57.543% train accuracy  15 -> 60.942% test accuracy\n",
      " 16 -> 58.484% train accuracy  16 -> 72.873% test accuracy\n",
      " 17 -> 59.440% train accuracy  17 -> 76.947% test accuracy\n",
      " 18 -> 59.558% train accuracy  18 -> 53.806% test accuracy\n",
      " 19 -> 59.259% train accuracy  19 -> 50.080% test accuracy\n",
      " 20 -> 59.007% train accuracy  20 -> 57.392% test accuracy\n",
      " 21 -> 58.805% train accuracy  21 -> 54.038% test accuracy\n",
      " 22 -> 58.622% train accuracy  22 -> 56.647% test accuracy\n",
      " 23 -> 58.746% train accuracy  23 -> 56.206% test accuracy\n",
      " 24 -> 59.318% train accuracy  24 -> 71.446% test accuracy\n",
      " 25 -> 59.856% train accuracy  25 -> 76.146% test accuracy\n",
      " 26 -> 60.284% train accuracy  26 -> 74.383% test accuracy\n",
      " 27 -> 60.868% train accuracy  27 -> 72.019% test accuracy\n",
      " 28 -> 61.456% train accuracy  28 -> 78.289% test accuracy\n",
      " 29 -> 61.698% train accuracy  29 -> 49.988% test accuracy\n",
      " 30 -> 61.532% train accuracy  30 -> 59.375% test accuracy\n",
      " 31 -> 61.588% train accuracy  31 -> 67.909% test accuracy\n",
      " 32 -> 61.524% train accuracy  32 -> 66.991% test accuracy\n",
      " 33 -> 61.825% train accuracy  33 -> 76.094% test accuracy\n",
      " 34 -> 62.278% train accuracy  34 -> 75.397% test accuracy\n",
      " 35 -> 62.748% train accuracy  35 -> 80.004% test accuracy\n",
      " 36 -> 63.216% train accuracy  36 -> 52.845% test accuracy\n",
      " 37 -> 63.024% train accuracy  37 -> 56.843% test accuracy\n",
      " 38 -> 63.023% train accuracy  38 -> 64.920% test accuracy\n",
      " 39 -> 62.865% train accuracy  39 -> 50.064% test accuracy\n",
      " 40 -> 62.819% train accuracy  40 -> 57.684% test accuracy\n",
      " 41 -> 62.695% train accuracy  41 -> 58.217% test accuracy\n",
      " 42 -> 62.940% train accuracy  42 -> 78.373% test accuracy\n",
      " 43 -> 62.982% train accuracy  43 -> 51.651% test accuracy\n",
      " 44 -> 62.823% train accuracy  44 -> 56.903% test accuracy\n",
      " 45 -> 62.755% train accuracy  45 -> 63.778% test accuracy\n",
      " 46 -> 62.914% train accuracy  46 -> 74.451% test accuracy\n",
      " 47 -> 63.222% train accuracy  47 -> 78.157% test accuracy\n",
      " 48 -> 63.531% train accuracy  48 -> 79.539% test accuracy\n",
      " 49 -> 63.843% train accuracy  49 -> 79.848% test accuracy\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_len = 300, hidden_size = 100, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size   = emb_len, \n",
    "            hidden_size  = hidden_size, \n",
    "            num_layers   = 2,\n",
    "            nonlinearity = 'relu',\n",
    "            batch_first  = True\n",
    "        )\n",
    "        self.hidden_layer    = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer    = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_transfer = nn.ReLU()\n",
    "        self.output_transfer = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        x = output[:,-1]\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_transfer(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.output_transfer(x)\n",
    "        return x\n",
    "    \n",
    "epochs    = 50\n",
    "model     = RNN()\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train(\n",
    "  model, loss_func, optimizer, \n",
    "  train_loader, test_loader, \n",
    "  epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d187b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 -> 53.870% train accuracy   0 -> 54.772% test accuracy\n",
      "  1 -> 54.419% train accuracy   1 -> 52.901% test accuracy\n",
      "  2 -> 57.282% train accuracy   2 -> 77.680% test accuracy\n",
      "  3 -> 62.706% train accuracy   3 -> 80.629% test accuracy\n",
      "  4 -> 66.462% train accuracy   4 -> 78.946% test accuracy\n",
      "  5 -> 69.145% train accuracy   5 -> 81.791% test accuracy\n",
      "  6 -> 71.124% train accuracy   6 -> 82.740% test accuracy\n",
      "  7 -> 72.699% train accuracy   7 -> 81.222% test accuracy\n",
      "  8 -> 73.972% train accuracy   8 -> 81.931% test accuracy\n",
      "  9 -> 75.058% train accuracy   9 -> 82.628% test accuracy\n",
      " 10 -> 76.045% train accuracy  10 -> 83.241% test accuracy\n",
      " 11 -> 76.912% train accuracy  11 -> 83.397% test accuracy\n",
      " 12 -> 77.670% train accuracy  12 -> 82.620% test accuracy\n",
      " 13 -> 78.408% train accuracy  13 -> 82.933% test accuracy\n",
      " 14 -> 79.102% train accuracy  14 -> 82.059% test accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[1;32m     37\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, optimizer, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     total_pred   \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     31\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 32\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m correct_pred \u001b[38;5;241m/\u001b[39m total_pred\n",
      "File \u001b[0;32m~/python-venv/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/python-venv/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/python-venv/lib/python3.11/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/python-venv/lib/python3.11/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python-venv/lib/python3.11/site-packages/torch/optim/adam.py:376\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    374\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[1;32m    379\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n",
      "File \u001b[0;32m~/python-venv/lib/python3.11/site-packages/torch/optim/optimizer.py:44\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, emb_size = 300, hidden_size = 100, output_size = 2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size  = emb_size, \n",
    "            num_layers  = 1,\n",
    "            hidden_size = hidden_size, \n",
    "            batch_first = True\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer    = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer    = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_transfer = nn.ReLU()\n",
    "        self.output_transfer = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(1, len(x), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(1, len(x), self.hidden_size))\n",
    "        \n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        x = out[:,-1]\n",
    "        \n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_transfer(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.output_transfer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "epochs    = 25\n",
    "model     = LSTM()\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train(\n",
    "  model, loss_func, optimizer, \n",
    "  train_loader, test_loader, \n",
    "  epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a78e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, emb_size = 300, hidden_size = 100, output_size = 2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size    = emb_size, \n",
    "            num_layers    = 1,\n",
    "            hidden_size   = hidden_size, \n",
    "            batch_first   = True,\n",
    "            bidirectional = True\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer    = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.output_layer    = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.hidden_transfer = nn.ReLU()\n",
    "        self.output_transfer = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "        h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size))\n",
    "        \n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        x = out[:,-1]\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_transfer(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.output_transfer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "epochs    = 10\n",
    "model     = LSTM()\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train(\n",
    "  model, loss_func, optimizer, \n",
    "  train_loader, test_loader, \n",
    "  epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee79b48",
   "metadata": {},
   "source": [
    "#### Récurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "            \n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS()\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def build_vocabulary(dataset):\n",
    "    for _, text in dataset:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    build_vocabulary(train_dataset), \n",
    "    min_freq = 1, \n",
    "    specials = ['<unk>']\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b39c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "tokens  = tokenizer('RNN is a neural network that is able to understand sequences')\n",
    "indexes = vocab(tokens)\n",
    "print(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import torch\n",
    "\n",
    "train_dataset  = to_map_style_dataset(train_dataset)\n",
    "test_dataset   = to_map_style_dataset(test_dataset)\n",
    "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "max_words      = 100\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    y, X = list(zip(*batch))\n",
    "    X = [vocab(tokenizer(text)) for text in X]\n",
    "    \n",
    "    for index, tokens in enumerate(X):\n",
    "        if len(tokens) < max_words:\n",
    "            X[index] = tokens + ([0] * (max_words-len(tokens)))\n",
    "        else:\n",
    "            X[index] = tokens[:max_words]\n",
    "        \n",
    "    return torch.tensor(X, dtype = torch.int32), torch.tensor(y) - 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = 1024, \n",
    "    collate_fn = vectorize_batch, \n",
    "    shuffle    = True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size = 1024, \n",
    "    collate_fn = vectorize_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bc57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "embed_len  = 50\n",
    "hidden_dim = 50\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings = len(vocab), \n",
    "            embedding_dim  = embed_len\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size   = embed_len, \n",
    "            hidden_size  = hidden_dim, \n",
    "            num_layers   = 1,\n",
    "            nonlinearity = 'relu',\n",
    "            batch_first  = True\n",
    "        )\n",
    "        self.linear   = nn.Linear(\n",
    "            hidden_dim, \n",
    "            len(target_classes)\n",
    "        )\n",
    "        self.output_transfer = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        x = output[:,-1]\n",
    "        x = self.linear(x)\n",
    "        return self.output_transfer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584077bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "  correct_pred = 0\n",
    "  total_pred   = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "      y_pred        = model(X)\n",
    "      y_pred_class  = y_pred.argmax(dim = 1)\n",
    "      correct_pred += (y_pred_class == y).sum().item()\n",
    "      total_pred   += len(y)\n",
    "\n",
    "  return correct_pred / total_pred\n",
    "\n",
    "def train(\n",
    "  model, loss_func, optimizer, train_loader,\n",
    "  test_loader, epochs\n",
    "):\n",
    "  for epoch in range(epochs):\n",
    "    for X, y in train_loader:\n",
    "      y_pred = model(X)\n",
    "      loss   = loss_func(y_pred, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    train_accuracy = evaluate(model, train_loader)\n",
    "    test_accuracy  = evaluate(model, test_loader)\n",
    "    print(\n",
    "      f'{epoch:3} -> {100 * train_accuracy:5.3f}% train accuracy',\n",
    "      f'{epoch:3} -> {100 * test_accuracy:5.3f}% test accuracy'\n",
    "    )\n",
    "\n",
    "epochs        = 20\n",
    "learning_rate = 1e-3\n",
    "model         = RNN()\n",
    "loss_func     = nn.NLLLoss()\n",
    "optimizer     = optim.Adam(\n",
    "  params = model.parameters(),\n",
    "  lr     = learning_rate\n",
    ")\n",
    "train(\n",
    "  model, loss_func, optimizer, \n",
    "  train_loader, test_loader, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237302f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def make_predictions(model, test_loader):\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            preds = model(X)\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(y)\n",
    "\n",
    "    y_true, y_pred = torch.cat(y_true), torch.cat(y_pred)\n",
    "    y_pred         = y_pred.argmax(dim = -1)\n",
    "    y_true, y_pred = y_true.numpy(), y_pred.numpy()\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "y_true, y_pred = make_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(f\"accuracy on testset : {accuracy_score(y_true, y_pred)}\")\n",
    "print(\"\\nclassification report : \")\n",
    "print(classification_report(y_true, y_pred, target_names = target_classes))\n",
    "print(\"\\nconfusion matrix : \")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(\n",
    "    [target_classes[i] for i in y_true], \n",
    "    [target_classes[i] for i in y_pred],\n",
    "    normalize  = True,\n",
    "    title      = \"Confusion Matrix\",\n",
    "    cmap       = \"Purples\",\n",
    "    hide_zeros = True,\n",
    "    figsize    = (5,5)\n",
    ");\n",
    "plt.xticks(rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "def vectorize_text(text):\n",
    "    tokens_idx = vocab(tokenizer(text))\n",
    "\n",
    "    if len(tokens_idx) < max_words:\n",
    "        x = tokens_idx + ([0] * (max_words - len(tokens_idx)))\n",
    "    else:\n",
    "        x = tokens_idx[:max_words]\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_targets_proba(texts):\n",
    "    X    = [vectorize_text(text) for text in texts]\n",
    "    pred = model(torch.tensor(X, dtype = torch.int32))\n",
    "    pred = torch.exp(pred)\n",
    "    return pred.detach().numpy()\n",
    "\n",
    "label, text = test_dataset[0]\n",
    "label      -= 1\n",
    "\n",
    "explainer = lime_text.LimeTextExplainer(\n",
    "    class_names = target_classes, \n",
    "    verbose     = True\n",
    ")\n",
    "\n",
    "x = vectorize_text(text)\n",
    "x = torch.tensor([x], dtype = torch.int32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(x).argmax(dim = -1)[0]\n",
    "\n",
    "print(\"prediction : \", target_classes[pred])\n",
    "print(\"truth :      \", target_classes[label])\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    text, \n",
    "    classifier_fn = get_targets_proba,\n",
    "    labels        = [label]\n",
    ")\n",
    "\n",
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, emb_size = 300, hidden_size = 64):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size  = emb_size, \n",
    "            hidden_size = hidden_size, \n",
    "            batch_first = True\n",
    "        )\n",
    "        \n",
    "        self.lstm_transfer   = nn.ReLU()\n",
    "        self.hidden_layer    = nn.Linear(hidden_size, 64)\n",
    "        self.output_layer    = nn.Linear(64, 2)\n",
    "        self.hidden_transfer = nn.ReLU()\n",
    "        self.output_transfer = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(1, len(x), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(1, len(x), self.hidden_size))\n",
    "        \n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        x = self.lstm_transfer(h_n)\n",
    "        x = x.reshape(-1, self.hidden_size)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_transfer(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.output_transfer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21144354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sentiment_dataset.pkl', 'rb') as fs:\n",
    "    train_dataset, test_dataset = pickle.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea31800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def evaluate(model, dataset, batch_size):\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    correct_pred = 0\n",
    "    total_pred   = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y_pred        = model(X)\n",
    "            y_pred_class  = y_pred.argmax(dim = 1)\n",
    "            correct_pred += (y_pred_class == y).sum().item()\n",
    "            total_pred   += len(y)\n",
    "\n",
    "    return correct_pred / total_pred\n",
    "\n",
    "def train(\n",
    "  model, loss_func, optimizer, train_dataset,\n",
    "  test_dataset, epochs, batch_size\n",
    "):\n",
    "    dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in dataloader:\n",
    "            y_pred = model(X)\n",
    "            loss   = loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_accuracy = evaluate(model, train_dataset, batch_size)\n",
    "        test_accuracy  = evaluate(model, test_dataset, batch_size)\n",
    "        print(\n",
    "            f'{epoch:3} -> {100 * train_accuracy:5.3f}% train accuracy',\n",
    "            f'{epoch:3} -> {100 * test_accuracy:5.3f}% test accuracy'\n",
    "        )\n",
    "\n",
    "epochs        = 50\n",
    "learning_rate = 1e-3\n",
    "batch_size    = 64\n",
    "model         = LSTM()\n",
    "loss_func     = nn.NLLLoss()\n",
    "optimizer     = optim.Adam(\n",
    "  params = model.parameters(),\n",
    "  lr     = learning_rate\n",
    ")\n",
    "\n",
    "train(\n",
    "  model, loss_func, optimizer, \n",
    "  train_dataset, test_dataset, \n",
    "  epochs, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a651966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers          import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "summarizer = pipeline('summarization', model = 'philschmid/distilbart-cnn-12-6-samsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd0e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Hurricane Ian continues to rain destruction onto the Florida. \n",
    "Millions have been left dark in the Sunshine State and residents \n",
    "along the low-lying peninsula’s many barrier islands have been \n",
    "cut off from the mainland as vehicles and whole roads were \n",
    "swallowed by the massive Category 4 storm.\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(\n",
    "    text, \n",
    "    min_length = 10, \n",
    "    max_length = 25\n",
    ")[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "zero_shot_text_classificiation = pipeline(\n",
    "    'zero-shot-classification', \n",
    "    model = 'valhalla/distilbart-mnli-12-6'\n",
    ")\n",
    "\n",
    "def argmax(alist):\n",
    "    return max(enumerate(alist), key = itemgetter(1))\n",
    "\n",
    "def text_classification(text):\n",
    "    res      = zero_shot_text_classificiation(text, classes)\n",
    "    index, _ = argmax(res['scores'])\n",
    "    label    = res['labels'][index]\n",
    "    return label\n",
    "\n",
    "classes = [\n",
    "      'This product description refers to a book or an ebook',\n",
    "      'This product description does not refer to a book and does not refer to an ebook'\n",
    "]\n",
    "\n",
    "text_1 = \"This is the best seller book of the year, buy it now!\"\n",
    "text_2 = \"This T-shirt will make you good looking and smarter everyday, buy it now!\"\n",
    "\n",
    "print(f'text_1: {text_classification(text_1)}')\n",
    "print(f'text_2: {text_classification(text_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels = 5\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding    = \"max_length\", \n",
    "        truncation = True\n",
    "    )\n",
    "\n",
    "tokenized_dataset   = dataset.map(tokenize_function, batched = True)\n",
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle().select(range(1000))\n",
    "small_eval_dataset  = tokenized_dataset[\"test\"].shuffle().select(range(1000))\n",
    "\n",
    "training_args     = TrainingArguments(\n",
    "    output_dir          = \"test_trainer\",\n",
    "    evaluation_strategy = \"epoch\"\n",
    ")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions    = np.argmax(logits, axis = -1)\n",
    "    return metric.compute(predictions = predictions, references = labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = small_train_dataset,\n",
    "    eval_dataset    = small_eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('SetFit/enron_spam')\n",
    "\n",
    "def clean_text(texts):\n",
    "    normalise_point  = r'\\.'\n",
    "    normalise_tags   = r'><'\n",
    "    unslash          = r'\\/'\n",
    "    space_norm_regex = r'\\s+'\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = re.sub(normalise_point, '. ', text)\n",
    "        cleaned_text = re.sub(unslash, ' ', cleaned_text)\n",
    "\n",
    "        cleaned_text = clean(cleaned_text,\n",
    "            fix_unicode                  = True,\n",
    "            to_ascii                     = True,\n",
    "            lower                        = True,\n",
    "            no_line_breaks               = True,\n",
    "            no_urls                      = True,\n",
    "            no_emails                    = True,\n",
    "            no_phone_numbers             = True,\n",
    "            no_numbers                   = True,\n",
    "            no_digits                    = True,\n",
    "            no_currency_symbols          = True,\n",
    "            no_punct                     = False,\n",
    "            replace_with_punct           = \"\",\n",
    "            replace_with_url             = \"<url>\",\n",
    "            replace_with_email           = \"<email>\",\n",
    "            replace_with_phone_number    = \"<phone>\",\n",
    "            replace_with_number          = \"<number>\",\n",
    "            replace_with_digit           = \"<digit>\",\n",
    "            replace_with_currency_symbol = \"<cur>\",\n",
    "            lang                         = \"en\"\n",
    "        )\n",
    "\n",
    "        cleaned_text = re.sub(normalise_tags, ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(space_norm_regex, ' ', cleaned_text)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "\n",
    "    return cleaned_texts\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        clean_text(examples[\"text\"]), \n",
    "        padding    = \"max_length\", \n",
    "        truncation = True\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels = 2\n",
    ")\n",
    "\n",
    "tokenizer.add_tokens(['<url>', '<email>', '<phone>', '<number>', '<digit>', '<cur>'])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenized_dataset   = dataset.map(tokenize_function, batched = True)\n",
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle().select(range(1000))\n",
    "small_eval_dataset  = tokenized_dataset[\"test\"].shuffle().select(range(1000))\n",
    "\n",
    "training_args     = TrainingArguments(\n",
    "    output_dir          = \"test_trainer\",\n",
    "    evaluation_strategy = \"epoch\"\n",
    ")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = small_train_dataset,\n",
    "    eval_dataset    = small_eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba661c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset['train']))\n",
    "print(len(dataset['test']))\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c05fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def clean_text(texts):\n",
    "    normalise_point    = r'\\.'\n",
    "    normalise_tags     = r'><'\n",
    "    space_norm_regex   = r'\\s+'\n",
    "    user_regex         = r'@\\S+'\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = re.sub(normalise_point, '. ', text)\n",
    "\n",
    "        cleaned_text = clean(cleaned_text,\n",
    "            fix_unicode                  = True,\n",
    "            to_ascii                     = True,\n",
    "            lower                        = True,\n",
    "            no_line_breaks               = True,\n",
    "            no_urls                      = True,\n",
    "            no_emails                    = True,\n",
    "            no_phone_numbers             = True,\n",
    "            no_numbers                   = True,\n",
    "            no_digits                    = True,\n",
    "            no_currency_symbols          = True,\n",
    "            no_punct                     = False,\n",
    "            replace_with_punct           = \"\",\n",
    "            replace_with_url             = \"<url>\",\n",
    "            replace_with_email           = \"<email>\",\n",
    "            replace_with_phone_number    = \"<phone>\",\n",
    "            replace_with_number          = \"<number>\",\n",
    "            replace_with_digit           = \"<digit>\",\n",
    "            replace_with_currency_symbol = \"<cur>\",\n",
    "            lang                         = \"en\"\n",
    "        )\n",
    "        \n",
    "        cleaned_text = re.sub(user_regex, 'user', cleaned_text)\n",
    "        cleaned_text = re.sub(normalise_tags, ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(space_norm_regex, ' ', cleaned_text)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "\n",
    "    return cleaned_texts\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions    = np.argmax(logits, axis = -1)\n",
    "    return metric.compute(predictions = predictions, references = labels, average = 'macro')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        clean_text(examples[\"text\"]), \n",
    "        padding        = \"max_length\", \n",
    "        truncation     = True\n",
    "    )\n",
    "\n",
    "def remap_labels(sample):\n",
    "    if sample['feeling'] == 0:\n",
    "        return { 'label': 0, 'label_text': 'negative' }\n",
    "    else:\n",
    "        return { 'label': 1, 'label_text': 'positive' }\n",
    "\n",
    "dataset   = load_dataset('carblacac/twitter-sentiment-analysis')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bhadresh-savani/distilbert-base-uncased-sentiment-sst2')\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bhadresh-savani/distilbert-base-uncased-sentiment-sst2', \n",
    "    num_labels = 2\n",
    ")\n",
    "\n",
    "dataset = dataset.map(remap_labels)\n",
    "tokenizer.add_tokens(['<url>', '<email>', '<phone>', '<number>', '<digit>', '<cur>', 'user'])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenized_dataset   = dataset.map(tokenize_function, batched = True)\n",
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle().select(range(1000))\n",
    "small_eval_dataset  = tokenized_dataset[\"test\"].shuffle().select(range(1000))\n",
    "\n",
    "training_args     = TrainingArguments(\n",
    "    output_dir          = \"test_trainer\",\n",
    "    evaluation_strategy = \"epoch\"\n",
    ")\n",
    "metric = evaluate.load('recall')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = small_train_dataset,\n",
    "    eval_dataset    = small_eval_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52af060",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('SetFit/enron_spam')\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook/blenderbot-400M-distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3acd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "model_name = \"mrm8488/longformer-base-4096-finetuned-squadv2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa         = pipeline(\"question-answering\", model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7236508",
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook = \"To create a new account you need to : click on the sign up on the top right of the website; then fill the form with your full name, email address, shipping address, credit card and credit card information; then verify your email address by following the instructions in the email you've just received\"\n",
    "question = \"I've just filled the form but my account is not created. What should I do?\"\n",
    "\n",
    "qa({\"question\": question, \"context\": textbook})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a021a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('bitext/customer-support-intent-dataset')\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d515017",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len       = 256\n",
    "batch_size    = 8\n",
    "epochs        = 3\n",
    "learning_rate = 1e-05\n",
    "\n",
    "alllabels = set()\n",
    "def map_labels(sample):\n",
    "    alllabels.add(sample['intent'])\n",
    "    alllabels.add(sample['category'])\n",
    "    \n",
    "    tags = sample['tags']\n",
    "    for c in tags:\n",
    "        alllabels.add(c)\n",
    "        \n",
    "dataset.map(map_labels)\n",
    "\n",
    "index     = 0\n",
    "label_map = dict()\n",
    "for label in alllabels:\n",
    "    label_map[label] = index\n",
    "    index           += 1\n",
    "\n",
    "def remap_labels(sample):\n",
    "    labels = [0] * len(label_map)\n",
    "    labels[label_map[sample['intent']]]   = 1\n",
    "    labels[label_map[sample['category']]] = 1\n",
    "    \n",
    "    tags = sample['tags']\n",
    "    for c in tags:\n",
    "        labels[label_map[c]] = 1\n",
    "\n",
    "    return { 'label': labels }\n",
    "\n",
    "dataset = dataset.map(remap_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327268f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts     = dataset.data['utterance']\n",
    "        self.targets   = dataset.data['label']\n",
    "        self.max_len   = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text   = self.texts[index].as_py()\n",
    "        target = self.targets[index].as_py()\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length         = self.max_len,\n",
    "            padding            = 'max_length'\n",
    "        )\n",
    "        \n",
    "        ids  = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        X = [\n",
    "            torch.tensor(ids, dtype = torch.long),\n",
    "            torch.tensor(mask, dtype = torch.long)\n",
    "        ]\n",
    "        y = torch.tensor(target, dtype = torch.float)\n",
    "\n",
    "        return X, y\n",
    " \n",
    "train_dataset = MultiLabelDataset(\n",
    "    dataset['train'].shuffle().select(range(1000)), \n",
    "    tokenizer, max_len\n",
    ")\n",
    "test_dataset  = MultiLabelDataset(\n",
    "    dataset['test'], \n",
    "    tokenizer, max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class MultiLabelDistilBERTClass(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelDistilBERTClass, self).__init__()\n",
    "        self.distilbert      = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.hidden          = nn.Linear(768, 768)\n",
    "        self.hidden_transfer = nn.ReLU()\n",
    "        self.dropout         = nn.Dropout(0.3)\n",
    "        self.output          = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_ids      = x[0]\n",
    "        attention_mask = x[1]\n",
    "        x = self.distilbert(\n",
    "            input_ids      = input_ids, \n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        \n",
    "        x = x[0][:, 0]\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden_transfer(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model     = MultiLabelDistilBERTClass(len(alllabels))\n",
    "optimizer = optim.Adam(\n",
    "    params = model.parameters(), \n",
    "    lr     = learning_rate\n",
    ")\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29832e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, batch_size):\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "    correct_pred = 0\n",
    "    total_pred   = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y_pred        = model(X)\n",
    "            y_pred_class  = y_pred.argmax(dim = 1)\n",
    "            y_class       = y.argmax(dim = 1)\n",
    "            correct_pred += (y_pred_class == y_class).sum().item()\n",
    "            total_pred   += len(y)\n",
    "\n",
    "    return correct_pred / total_pred\n",
    "\n",
    "def train(\n",
    "  model, loss_func, optimizer, train_dataset,\n",
    "  test_dataset, epochs, batch_size\n",
    "):\n",
    "    dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in dataloader:\n",
    "            y_pred = model(X)\n",
    "            loss   = loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_accuracy = evaluate(model, train_dataset, batch_size)\n",
    "        test_accuracy  = evaluate(model, test_dataset, batch_size)\n",
    "        print(\n",
    "            f'{epoch:3} -> {100 * train_accuracy:5.3f}% train accuracy',\n",
    "            f'{epoch:3} -> {100 * test_accuracy:5.3f}% test accuracy'\n",
    "        )\n",
    "        \n",
    "train(\n",
    "  model, loss_func, optimizer, train_dataset,\n",
    "  test_dataset, epochs, batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e739d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
